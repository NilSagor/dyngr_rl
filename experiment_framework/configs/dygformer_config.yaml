# DyGFormer Model Configuration

# Model parameters
model:
  name: "DyGFormer"
  num_nodes: null  # Will be set based on dataset
  node_features: 0  # Use learned embeddings
  hidden_dim: 176
  time_encoding_dim: 32
  num_layers: 2
  num_heads: 4 # 1, 2, 4, 8
  dropout: 0.1
  max_neighbors: 20
  neighbor_co_occurrence: true
  
# Training parameters
training:
  batch_size: 200
  learning_rate: 0.0001
  weight_decay: 0.00001
  max_epochs: 100
  patience: 5
  gradient_clip_val: 1.0
  
# Data parameters
data:
  dataset: "reddit"
  evaluation_type: inductive
  negative_sampling_strategy: "historical"
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  max_neighbors: 20
  
# Experiment parameters
experiment:
  name: "dygformer_experiment"
  description: "DyGFormer transformer experiment"
  seed: 42
  device: "auto"
  precision: 32
  
# Logging
logging:
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  
# Evaluation
evaluation:
  metrics: ["accuracy", "ap", "auc"]
  test_batch_size: 1000
  
# Hardware
hardware:
  gpus: 1
  num_workers: 4
  pin_memory: true