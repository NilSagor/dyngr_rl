# DyGFormer Model Configuration

# Model parameters
model:
  name: "DyGFormer"
  num_nodes: null  # Will be set based on dataset
  node_features: 172  # wikipedia had 172 D features, if not, use 0 Use learned embeddings
  hidden_dim: 176
  time_encoding_dim: 32
  num_layers: 2
  num_heads: 4
  dropout: 0.1
  max_neighbors: 20
  patch_size: 1
  max_sequence_length: 256
  channel_embedding_dim: 64  # New parameter
  neighbor_co_occurrence: true
  
# Training parameters
training:
  batch_size: 200 # for debugging 32, 200
  learning_rate: 0.0001
  weight_decay: 0.00001
  max_epochs: 100
  patience: 5
  gradient_clip_val: 1.0
  
# Data parameters
data:
  dataset: "wikipedia"
  # overriden 
  evaluation_type: random # change to "inductive" for inductive 
  # negative_sampling_strategy: "transductive" # random, historical/inductive
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  max_neighbors: 20
  
# Experiment parameters
experiment:
  # name: "${model.name}_${data.evaluation_type}_${data.negative_sampling_strategy}"
  # description: "${model.name} on ${data.dataset} |${data.evaluation_type}|${data.negative_sampling_strategy}"
  seed: 42
  device: "auto"
  precision: 32


  
# Logging
logging:
  # log_dir: "./logs/${experiment.name}"
  # checkpoint_dir: "./checkpoints/${experiment.name}"
  save_top_k: 3
  monitor: "val_loss"
  mode: "min"
  
# Evaluation
evaluation:
  metrics: ["accuracy", "ap", "auc"]
  test_batch_size: 1000 #1000 
  
# Hardware
hardware:
  gpus: 1
  num_workers: 0
  pin_memory: false

